LUNA16_ResNet50V2_Experiment_1 - Čistý ResNet50V2 bez úprav, Adam(lr = 0.001)
LUNA16_ResNet50V2_Experiment_2 - Čistý ResNet50V2 bez úprav, Adam(lr = 0.0001)
LUNA16_ResNet50V2_Experiment_3 - Čistý ResNet50V2 bez úprav, Adam(lr = 0.00001)

LUNA16_ResNet50_Dropout_Experiment_1 - ResNet50V2 + 0.5 dropout za ResNet sietou, Adam(lr = 0.001)
LUNA16_ResNet50_Dropout_Experiment_2 - ResNet50V2 + 0.5 dropout za ResNet sietou, Adam(lr = 0.0001)
LUNA16_ResNet50_Dropout_Experiment_3 - ResNet50V2 + 0.5 dropout za ResNet sietou, Adam(lr = 0.00001)

LUNA16_DenseNet121_Experiment_1 - Čistý DenseNet121 bez úprav, Adam(lr = 0.001)
LUNA16_DenseNet121_Experiment_2 - Čistý DenseNet121 bez úprav, Adam(lr = 0.0001)

LUNA16_Arch1_Experiment_1 - Mnou navrhnutá sieť využívajúca 4 konvolučné bloky a plne-prepojené vrstvy inšpirované VGG19. Adam(lr = 0.0001)
LUNA16_Arch1_Experiment_2 - Mnou navrhnutá sieť využívajúca 4 konvolučné bloky a plne-prepojené vrstvy inšpirované VGG19. Adam(lr = 0.00001)
LUNA16_Arch1_Experiment_3 - Mnou navrhnutá sieť využívajúca 4 konvolučné bloky a plne-prepojené vrstvy inšpirované VGG19. Adam(lr = 0.000015)
LUNA16_Arch1_Experiment_4 - LUNA16_Arch1_Experiment_2 mal dobré výsledky, zdá sa že Learning Rate je priateľný. Pridávam Dropout na dense vrstvy ako pokus pri zabránení overfittingu. Adam(lr = 0.00001)
LUNA16_Arch1_Experiment_5 - Rovnaká architektúra ako Experiment 4, zvýšenie EarlyStop patience na 10 epoch. Adam(lr = 0.00001)
LUNA16_Arch1_Experiment_6 - Mnou navrhnutá sieť využívajúca 4 konvolučné bloky a plne-prepojené vrstvy inšpirované VGG19. Adam(lr = 0.000001)
LUNA16_Arch1_Experiment_7 - Mnou navrhnutá sieť využívajúca 4 konvolučné bloky a plne-prepojené vrstvy inšpirované VGG19. Zvýšený LR. Adam(lr = 0.000005)
LUNA16_Arch1_Experiment_8 - LUNA16_Arch1_Experiment_7 s pridaným 0.25 dropout na plne-prepojených vrstvách. Adam(lr = 0.000005)
LUNA16_Arch1_Experiment_9 - LUNA16_Arch1_Experiment_7 s pridaným 0.50 dropout na plne-prepojených vrstvách. Adam(lr = 0.000005)
LUNA16_Arch1_Experiment_10 - LUNA16_Arch1_Experiment_7 s pridaným 0.75 dropout na plne-prepojených vrstvách. Adam(lr = 0.000005)
LUNA16_Arch1_Experiment_11 - LUNA16_Arch1_Experiment_7 model s pridaným BatchNormalization za každou vrstvou. Adam(lr = 0.000005)

LUNA16_Arch1_ExtDat_Experiment_1 - LUNA16_Arch1_Experiment_7 tréning pomocou extended datasetu. Adam(lr = 0.000005)

TODO:
LUNA16_DenseNet121_Experiment_3 - Čistý DenseNet121 bez úprav, Adam(lr = 0.00001)

